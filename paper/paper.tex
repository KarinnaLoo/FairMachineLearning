%
% File acl2015.tex
%
% Contact: car@ir.hit.edu.cn, gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{amsmath}

\usepackage[labelsep=quad,indention=10pt]{subfig}
\captionsetup*[subfigure]{position=bottom}

\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\usepackage{graphicx}
\graphicspath{{figures/}}
\DeclareGraphicsExtensions{.eps,.pdf,.jpg,.png}

\DeclareMathOperator{\wsim}{sim}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{Title}

\author{JT Cho \\
	{\tt joncho@} \\
	{\tt seas.upenn.edu} \\\And
	Karinna Loo \\
	{\tt kloo@} \\
	{\tt seas.upenn.edu} \\\And
  	Veronica Wharton \\
  	{\tt whartonv@} \\
	{\tt seas.upenn.edu} }
\date{}

\begin{document}
\maketitle

%\begin{abstract}

%\noindent TODO: Abstract 

%\end{abstract}

\section{Introduction}

For our CIS 625 final project, our team --- JT Cho, Karinna Loo, and Veronica Wharton --- took a closer look at the topic of fairness in machine learning. The paper that piqued our interest was \textit{Rawlsian Fairness for Machine learning} \cite{DBLP:journals/corr/JosephKMNR16}, which describes two online algorithms in the linear contextual bandit framework that both learn at a rate comparable to (but necessarily worse than) the best algorithms absent of a fairness constraint and also satisfy a specified fairness constraint. The authors present theoretical and empirical results. Our team sought to re-implement the algorithms presented by \newcite{DBLP:journals/corr/JosephKMNR16} and then expand upon their empirical analyses. We were also interested in exploring further fairness analyses using real-world data.

\section{Project overview}

Our project consisted of the following steps:

\begin{enumerate}	\item We read the paper \textit{Rawlsian Fairness for Machine Learning}  \cite{DBLP:journals/corr/JosephKMNR16}.	\item We implemented the \texttt{TopInterval}, \texttt{IntervalChaining}, and \texttt{RidgeFair} algorithms from the paper in Python. 	\item We ran our implementations on a Yahoo! dataset containing a fraction of the user click log for news articles displayed in the Featured Tab of the Today Module on the Yahoo! Front Page during the first ten days in May 2009 \cite{yahoo}, to see how well they performed on real data.	\item To empirically evaluate our implementations, we ran experiments similar to those in \cite{DBLP:journals/corr/JosephKMNR16} with randomly-drawn contexts. 	\item We compiled our findings into a written report.
\end{enumerate}

\section{Algorithm implementations}

The code for our implementations can be found here: \url{https://github.com/jtcho/FairMachineLearning/blob/master/fairml.py}

All algorithms and code were written using Python 3, along with numpy, SciPy, and various other Python libraries.

\section{Implementation: TopInterval}

The \texttt{TopInterval} learning algorithm was implemented true to form as presented in \cite{DBLP:journals/corr/JosephKMNR16}. Particular details of note - to ensure that all matrices used in computation were nonsingular, the first $d$ rounds are always chosen to be exploration rounds, where $d$ is the number of features.

\section{Implementation: IntervalChaining}

The implementation for \texttt{IntervalChaining} was simple given \texttt{TopInterval}, as it sufficed to alter the strategy for picking arms in each round to that of picking uniformly at random from the chain containing the top interval.

\section{Implementation: RidgeFair}

\section{Yahoo! Dataset}

\section{Experimental results}

We ran experiments that compared the regret of \textsc{IntervalChaining} (IC) with the regret of \textsc{TopInterval} (TI). As in \newcite{DBLP:journals/corr/JosephKMNR16}, we present three sets of empirical results: 
\begin{itemize}
	\item Varying $T$ (the number of rounds) - we measured the average regret of \textsc{IntervalChaining} and \textsc{TopInterval} as a function of increasing $T$.
	\item Varying $k$ (the number of arms/groups) - we measured the average regret of \textsc{IntervalChaining} and \textsc{TopInterval} as a function of increasing $k$.
	\item Varying $d$ (the number of features) - we measured the average regret of \textsc{IntervalChaining} and \textsc{TopInterval} as a function of increasing $d$.
\end{itemize} 

For each increasing variable ($T$, $k$, or $d$), we present nine metrics as a function of the variable, each averaged over 500 trials. Contexts are drawn uniformly at random from $[0,1]^d$ and standard Gaussian noise. \newcite{DBLP:journals/corr/JosephKMNR16} only present the average regret difference (metric \#3).
\begin{enumerate}
	\item Average regret (TI) - the average regret of \textsc{TopInterval} across all rounds.
	\item Average regret (IC) - the average regret of \textsc{IntervalChaining} across all rounds.
	\item Average regret difference (TI vs. IC) - the difference between the average regrets of \textsc{TopInterval} and \textsc{IntervalChaining} across all rounds.
	\item Cumulative regret (TI) - the cumulative regret of \textsc{TopInterval} across all rounds.
	\item Cumulative regret (IC) - the cumulative regret of \textsc{IntervalChaining} across all rounds.
	\item Cumulative regret difference (TI vs. IC) - the difference between the cumulative regrets of \textsc{TopInterval} and \textsc{IntervalChaining} across all rounds.
	\item Final regret (TI) - the regret of \textsc{TopInterval} in the final round.
	\item Final regret (IC) - the regret of \textsc{IntervalChaining} in the final round.
	\item Final regret difference (TI vs. IC) - the difference between the final regrets of \textsc{TopInterval} and \textsc{IntervalChaining}.
\end{enumerate}

\section{Conclusion}

\bibliography{paper}
\bibliographystyle{acl}

\end{document}
