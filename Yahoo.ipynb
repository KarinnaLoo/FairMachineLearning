{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data wrangling for the Yahoo! Front Page Today Module User Click Log Dataset, version 1.0.\n",
    "\n",
    "Inspired by:\n",
    "Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation Algorithms \n",
    "[https://arxiv.org/pdf/1003.5956.pdf]\n",
    "\n",
    "Documentation is per reST format used in Sphinx.\n",
    "\n",
    "Dataset: https://webscope.sandbox.yahoo.com/catalog.php?datatype=r&did=49\n",
    "Author: jtcho (jonathan.t.cho@gmail.com)\n",
    "\n",
    "Many thanks to Yahoo! Research for allowing me to use their dataset.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import time\n",
    "import os\n",
    "\n",
    "dump_dir = 'R6/'\n",
    "data_dirs = ['clicks_1/']\n",
    "engine = sqlite3.connect('yahoo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Database cleanup.\n",
    "\n",
    "c = engine.cursor()\n",
    "c.execute('DROP TABLE articles')\n",
    "engine.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_article_info(path, item_limit=sys.maxsize):\n",
    "    \"\"\" \n",
    "    Given an R6A dataset file, extracts all of the common article vectors\n",
    "    and compiles them in a single dataframe.\n",
    "    Note that each article has a constant vector associated with it.\n",
    "    \n",
    "    :param path:       the file path for the dataset\n",
    "    :param item_limit: limits the number of items to parse\n",
    "    :returns: Pandas dataframe containing article vectors indexed by id\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    num_iters = 0\n",
    "    _articles_df = pd.DataFrame(columns=['2', '3', '4', '5', '6', '1'])\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            num_iters += 1 \n",
    "            if num_iters > item_limit:\n",
    "                break\n",
    "            parts = line.strip().split('|')\n",
    "            for i in range(2, len(parts)):\n",
    "                # Extract article vector information.\n",
    "                article_info = parts[i].split()\n",
    "                article_id = article_info[0]\n",
    "                if article_id in _articles_df.index:\n",
    "                    continue\n",
    "                article_info_parts = list(map(lambda x : x.split(':')[1], article_info[1:]))\n",
    "                article_info = dict(zip(_articles_df.columns, article_info_parts))\n",
    "                # I append to an existing DF for quick de-duplication. Also\n",
    "                # empirically, I observed that there is a small number of unique\n",
    "                # articles for any dataset, so the overhead of doing this is minimized.\n",
    "                _articles_df.loc[article_id] = pd.Series(article_info)\n",
    "\n",
    "    t1 = time.time()\n",
    "    print('Finished processing {0} items in {1} seconds.'.format(num_iters-1, t1 - t0))\n",
    "    return _articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_click_file(path, item_limit=sys.maxsize):\n",
    "    \"\"\"\n",
    "    Given an R6A dataset file, parses all of the view event logs and \n",
    "    compiles them in a single dataframe.\n",
    "    \n",
    "    A single view event consists of a unix timestamp, a 6-dimensional vector of\n",
    "    features describing the user, a set of 20 articles in the article pool\n",
    "    (the 20 arms of the multi-arm bandit), the id of the article displayed, and\n",
    "    a boolean marking whether the article was clicked.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    num_iters = 0\n",
    "    views_cols = ['time', 'user_1', 'user_2', 'user_3', 'user_4', 'user_5', 'user_6', \n",
    "                  'article_pool', 'displayed', 'clicked']\n",
    "    views = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            num_iters += 1\n",
    "            if num_iters > item_limit:\n",
    "                break\n",
    "            parts = line.strip().split('|')\n",
    "            unix_timestamp, disp_article_id, clicked = parts[0].split()\n",
    "            user_info = list(map(lambda x : x.split(':')[1], parts[1].split()[1:]))\n",
    "            user_info = dict(zip(views_cols[1:7], user_info))\n",
    "            user_info['time'] = unix_timestamp\n",
    "            user_info['displayed'] = disp_article_id\n",
    "            user_info['clicked'] = clicked\n",
    "    \n",
    "            # Extract article vector information.\n",
    "            article_ids = [parts[i].split()[0] for i in range(2, len(parts))]\n",
    "            user_info['article_pool'] = article_ids\n",
    "            # In this case, we construct the DF at the end because we're creating a new row\n",
    "            # for *every* item... over ~4 million items that becomes very expensive!\n",
    "            views.append(user_info)\n",
    "\n",
    "    t1 = time.time()\n",
    "    print('{0}: Finished processing {1} items in {2} seconds.'.format(path, num_iters-1, t1 - t0))\n",
    "    return pd.DataFrame(views, columns=views_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 4681991 items in 150.5566005706787 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Run to populate the articles table.\n",
    "articles_df = extract_article_info(dump_dir + 'clicks_1.txt', sys.maxsize).apply(pd.to_numeric)\n",
    "articles_df.to_sql('articles', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks_1/xaa: Finished processing 99999 items in 3.1617259979248047 seconds.\n",
      "clicks_1/xab: Finished processing 99999 items in 3.2025344371795654 seconds.\n",
      "clicks_1/xac: Finished processing 99999 items in 3.3164455890655518 seconds.\n",
      "clicks_1/xad: Finished processing 99999 items in 3.380336046218872 seconds.\n",
      "clicks_1/xae: Finished processing 99999 items in 3.0821828842163086 seconds.\n",
      "clicks_1/xaf: Finished processing 99999 items in 3.1906492710113525 seconds.\n",
      "clicks_1/xag: Finished processing 99999 items in 3.3087258338928223 seconds.\n",
      "clicks_1/xah: Finished processing 99999 items in 3.2571945190429688 seconds.\n",
      "clicks_1/xai: Finished processing 99999 items in 3.278446674346924 seconds.\n",
      "clicks_1/xaj: Finished processing 99999 items in 3.2920501232147217 seconds.\n",
      "clicks_1/xak: Finished processing 99999 items in 3.431187629699707 seconds.\n",
      "clicks_1/xal: Finished processing 99999 items in 3.40493106842041 seconds.\n",
      "clicks_1/xam: Finished processing 99999 items in 3.1150004863739014 seconds.\n",
      "clicks_1/xan: Finished processing 99999 items in 3.1503725051879883 seconds.\n",
      "clicks_1/xao: Finished processing 99999 items in 3.3162639141082764 seconds.\n",
      "clicks_1/xap: Finished processing 99999 items in 3.09061598777771 seconds.\n",
      "clicks_1/xaq: Finished processing 99999 items in 3.4392073154449463 seconds.\n",
      "clicks_1/xar: Finished processing 99999 items in 3.443249464035034 seconds.\n",
      "clicks_1/xas: Finished processing 99999 items in 3.5337443351745605 seconds.\n",
      "clicks_1/xat: Finished processing 99999 items in 3.4647445678710938 seconds.\n",
      "clicks_1/xau: Finished processing 99999 items in 3.6430513858795166 seconds.\n",
      "clicks_1/xav: Finished processing 99999 items in 3.6271255016326904 seconds.\n",
      "clicks_1/xaw: Finished processing 99999 items in 3.309832811355591 seconds.\n",
      "clicks_1/xax: Finished processing 99999 items in 3.460949420928955 seconds.\n",
      "clicks_1/xay: Finished processing 99999 items in 3.426335573196411 seconds.\n",
      "clicks_1/xaz: Finished processing 99999 items in 3.510620594024658 seconds.\n",
      "clicks_1/xba: Finished processing 99999 items in 3.6194756031036377 seconds.\n",
      "clicks_1/xbb: Finished processing 99999 items in 3.7689321041107178 seconds.\n",
      "clicks_1/xbc: Finished processing 99999 items in 3.7527005672454834 seconds.\n",
      "clicks_1/xbd: Finished processing 99999 items in 3.559547185897827 seconds.\n",
      "clicks_1/xbe: Finished processing 99999 items in 3.664827585220337 seconds.\n",
      "clicks_1/xbf: Finished processing 99999 items in 3.7467215061187744 seconds.\n",
      "clicks_1/xbg: Finished processing 99999 items in 3.2975916862487793 seconds.\n",
      "clicks_1/xbh: Finished processing 99999 items in 3.1932389736175537 seconds.\n",
      "clicks_1/xbi: Finished processing 99999 items in 3.480050802230835 seconds.\n",
      "clicks_1/xbj: Finished processing 99999 items in 3.307481050491333 seconds.\n",
      "clicks_1/xbk: Finished processing 99999 items in 3.3213932514190674 seconds.\n",
      "clicks_1/xbl: Finished processing 99999 items in 3.602836847305298 seconds.\n",
      "clicks_1/xbm: Finished processing 99999 items in 3.3665266036987305 seconds.\n",
      "clicks_1/xbn: Finished processing 99999 items in 3.5517754554748535 seconds.\n",
      "clicks_1/xbo: Finished processing 99999 items in 3.5413339138031006 seconds.\n",
      "clicks_1/xbp: Finished processing 99999 items in 3.082970380783081 seconds.\n",
      "clicks_1/xbq: Finished processing 99999 items in 3.1382272243499756 seconds.\n",
      "clicks_1/xbr: Finished processing 99999 items in 3.2157583236694336 seconds.\n",
      "clicks_1/xbs: Finished processing 99999 items in 3.396573543548584 seconds.\n",
      "clicks_1/xbt: Finished processing 99999 items in 3.4965860843658447 seconds.\n",
      "clicks_1/xbu: Finished processing 81991 items in 2.8793578147888184 seconds.\n"
     ]
    }
   ],
   "source": [
    "for fname in os.listdir('clicks_1'):\n",
    "    if fname != '.DS_Store':\n",
    "        result = process_click_file('clicks_1/'+fname)\n",
    "        result['article_pool'] = result['article_pool'].astype(str)\n",
    "        result.to_sql('clicks', engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd.read_sql_query('select * from articles',con=engine).set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4681992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(*)\n",
       "0   4681992"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query('select count(*) from clicks', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
